{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "a_WypuUXi92e",
    "outputId": "133d026e-4236-4ff6-f21d-739bfb9640db"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import keras \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM, Bidirectional, Embedding, Dropout, BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras_multi_head import MultiHead\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import json \n",
    "import csv \n",
    "import os \n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "\n",
    "def fxn():\n",
    "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_file_name = \"intent_model.h5\" \n",
    "epoch_number1 = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_csv_file():\n",
    "    # Opening JSON file and loading the data \n",
    "    # into the variable data \n",
    "    with open('intents.json') as json_file: \n",
    "        data = json.load(json_file) \n",
    "  \n",
    "    intents_data = data['intents'] \n",
    "  \n",
    "    try:\n",
    "        os.remove(\"intents_data.csv\") \n",
    "    except:\n",
    "        pass \n",
    "\n",
    "    # open csv type file for writing \n",
    "    intent_data_file = open('intents_data.csv', 'w', newline='') \n",
    "  \n",
    "    # create the csv writer object \n",
    "    csv_writer = csv.writer(intent_data_file) \n",
    "  \n",
    "    # Counter variable used for writing headers to the CSV file \n",
    "    count = 0\n",
    "  \n",
    "    for intent in intents_data: \n",
    "        if count == 0: \n",
    "            print(intent)\n",
    "            # Writing headers of CSV file \n",
    "            header = intent.keys() \n",
    "            print(header)\n",
    "            csv_writer.writerow(header) \n",
    "            count += 1\n",
    "  \n",
    "        # Writing data of CSV file \n",
    "        csv_writer.writerow(intent.values()) \n",
    "        # file will be automatically properly closed in case of an Exception.\n",
    "        intent_data_file.flush() \n",
    "\n",
    "    intent_data_file.close() \n",
    "\n",
    "    data = pd.read_csv(\"intents_data.csv\").rename(columns={\"id\": \"target_intent_or_id\", \"query\": \"feature_sentence_or_query\"})\n",
    "    data = data[[\"feature_sentence_or_query\",\"target_intent_or_id\"]]\n",
    "    data.to_csv(\"intents_data.csv\", index=False)\n",
    "    data = pd.read_csv(\"intents_data.csv\")\n",
    "    print(\"process 1 . \")\n",
    "    print(data)\n",
    "  \n",
    "    data_store = open(\"intents_data.csv\")\n",
    "    input_file = csv.reader(data_store)\n",
    "\n",
    "    try:\n",
    "        del data \n",
    "    except:\n",
    "        pass \n",
    "\n",
    "    try:\n",
    "        os.remove(\"processed_intents_data.csv\") \n",
    "    except:\n",
    "        pass \n",
    "\n",
    "    count_check = 0\n",
    "    \n",
    "    df = pd.DataFrame(columns=[\"feature_sentence_or_query\", \"target_intent_or_id\"])\n",
    "    count_check = 0\n",
    "    for row in input_file:\n",
    "        if(count_check == 0):\n",
    "            count_check = count_check + 1\n",
    "            pass\n",
    "        else:\n",
    "            data_1 = list(row[0].replace(\"['\", \"\").replace(\"']\", '').replace(\" '\", \"\").split('\\','))\n",
    "            print(data_1)\n",
    "            for data in data_1:\n",
    "                df = df.append({\n",
    "            \"feature_sentence_or_query\": data,\n",
    "            \"target_intent_or_id\": row[1]  \n",
    "             }, ignore_index=True)\n",
    "        \n",
    "    df.to_csv(\"processed_intents_data.csv\", index=False)\n",
    "    data_store.close() \n",
    "    os.remove(\"intents_data.csv\") \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LE6wywJrN2ih"
   },
   "outputs": [],
   "source": [
    "def load_dataset(filename):\n",
    "  json_to_csv_file()\n",
    "  df = pd.read_csv(filename, encoding = \"latin1\", names = [\"feature_sentence_or_query\", \"target_intent_or_id\"],header = 1)\n",
    "  print(df.head())\n",
    "  intent = df[\"target_intent_or_id\"]\n",
    "  unique_intent = list(set(intent))\n",
    "  sentences = list(df[\"feature_sentence_or_query\"])\n",
    "  \n",
    "  return (intent, unique_intent, sentences)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j-7q3iG5PKYI"
   },
   "outputs": [],
   "source": [
    "def cleaning(sentences):\n",
    "  words = []\n",
    "  for s in sentences:\n",
    "    clean = re.sub(r'[^ a-z A-Z 0-9]', \" \", s)\n",
    "    w = word_tokenize(clean)\n",
    "    #stemming\n",
    "    words.append([i.lower() for i in w])\n",
    "    \n",
    "  return words  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SJCQ_YhBJW7t"
   },
   "outputs": [],
   "source": [
    "def create_tokenizer(words, filters = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~'):\n",
    "  token = Tokenizer(filters = filters)\n",
    "  token.fit_on_texts(words)\n",
    "  return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QJhdIJC5Q3Q6"
   },
   "outputs": [],
   "source": [
    "def max_length1(words):\n",
    "  return(len(max(words, key = len)))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D0TXu2xsR8jq"
   },
   "outputs": [],
   "source": [
    "def encoding_doc(token, words):\n",
    "  return(token.texts_to_sequences(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fyOzLEboc4LZ"
   },
   "outputs": [],
   "source": [
    "def padding_doc(encoded_doc, max_length):\n",
    "  return(pad_sequences(encoded_doc, maxlen = max_length, padding = \"post\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rD3QN-RPzfet"
   },
   "outputs": [],
   "source": [
    "def one_hot(encode):\n",
    "  o = OneHotEncoder(sparse = False)\n",
    "  return(o.fit_transform(encode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_and_target_data(intent, unique_intent, cleaned_words, word_tokenizer, vocab_size, max_length):\n",
    "    \n",
    "    print(\"Vocab Size = %d and Maximum length = %d\" % (vocab_size, max_length))\n",
    "    encoded_doc = encoding_doc(word_tokenizer, cleaned_words)\n",
    "    padded_doc = padding_doc(encoded_doc, max_length)\n",
    "    #tokenizer with filter changed\n",
    "    output_tokenizer = create_tokenizer(unique_intent, filters = '!\"#$%&()*+,-/:;<=>?@[\\]^`{|}~')\n",
    "    print(output_tokenizer.word_index)\n",
    "    encoded_output = encoding_doc(output_tokenizer, intent)\n",
    "    encoded_output = np.array(encoded_output).reshape(len(encoded_output), 1)\n",
    "    output_one_hot = one_hot(encoded_output)\n",
    "    feature = padded_doc\n",
    "    target = output_one_hot\n",
    "    return feature, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_insight():\n",
    "    intent, unique_intent, sentences = load_dataset(\"processed_intents_data.csv\")\n",
    "    nltk.download(\"stopwords\")\n",
    "    nltk.download(\"punkt\")\n",
    "    #define stemmer\n",
    "    stemmer = LancasterStemmer()\n",
    "    cleaned_words = cleaning(sentences)\n",
    "    word_tokenizer = create_tokenizer(cleaned_words)\n",
    "    vocab_size = len(word_tokenizer.word_index) + 1\n",
    "    max_length = max_length1(cleaned_words)\n",
    "    feature, target = get_feature_and_target_data(intent, unique_intent, cleaned_words, word_tokenizer, vocab_size, max_length)\n",
    "    clear_output()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(feature, target, shuffle = True, test_size = 0.1)\n",
    "    \n",
    "    print(\"Shape of X_train = %s and y_train = %s\" % (X_train.shape, y_train.shape))\n",
    "    print(\"Shape of X_test = %s and y_test = %s\" % (X_test.shape, y_test.shape))\n",
    "    return intent, unique_intent, cleaned_words, word_tokenizer, vocab_size, max_length, X_train, X_test, y_train, y_test \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e5BU_x74DNEb"
   },
   "outputs": [],
   "source": [
    "def create_model(vocab_size, max_length, y_train):\n",
    "  model = Sequential()\n",
    "  model.add(Embedding(vocab_size, 512, input_length = max_length, trainable = False))\n",
    "  model.add(MultiHead(LSTM(258), layer_num=6, name='Multi-LSTMs'))  \n",
    "  #model.add(Dropout(0.4))\n",
    "  # model.add(BatchNormalization())\n",
    "  #model.add(MultiHead([  \n",
    "  #  keras.layers.Conv1D(filters=512, kernel_size=1, padding='same'),\n",
    "  #  keras.layers.Conv1D(filters=512, kernel_size=1, padding='same'),\n",
    "  #  keras.layers.Conv1D(filters=512, kernel_size=1, padding='same'),\n",
    "  #    ], name='Multi-CNNs'))\n",
    "  #model.add(MultiHead(LSTM(512), layer_num=1, name='Multi-LSTMs'))  \n",
    "  # model.add(MultiHead(LSTM(128), layer_num=5, name='Multi-LSTMs'))\n",
    "  model.add(SeqSelfAttention(units=128, attention_width=16, attention_activation='sigmoid'))\n",
    "  # model.add(BatchNormalization())\n",
    "  model.add(Bidirectional(LSTM(128, return_sequences=True\n",
    "                       )))\n",
    "  #model.add(Dropout(0.001))\n",
    "  #model.add(BatchNormalization())\n",
    "  \n",
    "  model.add(keras.layers.Flatten(name='Flatten'))\n",
    "  model.add(Dense(128))  \n",
    "  #model.add(BatchNormalization())\n",
    "  model.add(Dense(32, activation = \"relu\"))\n",
    "  model.add(Dense(y_train.shape[1], activation = \"softmax\"))\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_intent_classifier_model(name, weights_file_name, epoch_number, vocab_size, max_length, X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    \n",
    "    print(\"\\n\\n\\n\", name, \" started . \\n\\n\\n\")\n",
    "    \n",
    "    m = create_model(vocab_size, max_length, y_train)\n",
    "\n",
    "    m.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "    clear_output()\n",
    "    print(m.summary())\n",
    "\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(weights_file_name, monitor='val_accuracy', verbose=1, save_weights_only=True, save_best_only=True, mode='max')\n",
    "\n",
    "    hist = m.fit(X_train, y_train, epochs = epoch_number, batch_size = 2, validation_data = (X_test, y_test), callbacks = [checkpoint])\n",
    "    \n",
    "    m = create_model(vocab_size, max_length, y_train)\n",
    "    m.load_weights(weights_file_name)\n",
    "    m.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "    \n",
    "    validation_loss, validation_accuracy  = m.evaluate(X_test, y_test)\n",
    "    \n",
    "    print(\"\\n\\n-------------------   Model Evaluation . ----------------------\\n\")\n",
    "    print(\" validation accuracy = \", validation_accuracy, \" , validation loss = \", validation_loss, \"\\n\")\n",
    "    print(90*\"-\")\n",
    "    return hist, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train = (22, 5) and y_train = (22, 8)\n",
      "Shape of X_test = (3, 5) and y_test = (3, 8)\n"
     ]
    }
   ],
   "source": [
    "intent, unique_intent, cleaned_words, word_tokenizer, vocab_size, max_length, \\\n",
    "X_train, X_test, y_train, y_test = data_insight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 5, 512)            19968     \n",
      "_________________________________________________________________\n",
      "Multi-LSTMs (MultiHead)      (None, 258, 6)            4774032   \n",
      "_________________________________________________________________\n",
      "seq_self_attention_1 (SeqSel (None, 258, 6)            1793      \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 258, 256)          138240    \n",
      "_________________________________________________________________\n",
      "Flatten (Flatten)            (None, 66048)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               8454272   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 264       \n",
      "=================================================================\n",
      "Total params: 13,392,697\n",
      "Trainable params: 13,372,729\n",
      "Non-trainable params: 19,968\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 22 samples, validate on 3 samples\n",
      "Epoch 1/100\n",
      "22/22 [==============================] - 23s 1s/step - loss: 2.2543 - accuracy: 0.0909 - val_loss: 2.3083 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.00000, saving model to intent_model.h5\n",
      "Epoch 2/100\n",
      "22/22 [==============================] - 13s 582ms/step - loss: 2.2058 - accuracy: 0.0909 - val_loss: 2.1243 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.00000\n",
      "Epoch 3/100\n",
      "22/22 [==============================] - 13s 581ms/step - loss: 2.3049 - accuracy: 0.2273 - val_loss: 2.4307 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.00000\n",
      "Epoch 4/100\n",
      "22/22 [==============================] - 13s 578ms/step - loss: 2.2785 - accuracy: 0.1364 - val_loss: 2.3997 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.00000\n",
      "Epoch 5/100\n",
      "22/22 [==============================] - 13s 577ms/step - loss: 2.1297 - accuracy: 0.1364 - val_loss: 2.0412 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.00000\n",
      "Epoch 6/100\n",
      "22/22 [==============================] - 13s 600ms/step - loss: 2.1173 - accuracy: 0.1818 - val_loss: 2.1857 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.00000 to 0.33333, saving model to intent_model.h5\n",
      "Epoch 7/100\n",
      "22/22 [==============================] - 13s 581ms/step - loss: 1.6796 - accuracy: 0.2727 - val_loss: 2.3847 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.33333\n",
      "Epoch 8/100\n",
      "22/22 [==============================] - 13s 592ms/step - loss: 1.6662 - accuracy: 0.3636 - val_loss: 2.7284 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.33333\n",
      "Epoch 9/100\n",
      "22/22 [==============================] - 13s 601ms/step - loss: 1.3458 - accuracy: 0.4091 - val_loss: 2.1711 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.33333\n",
      "Epoch 10/100\n",
      "22/22 [==============================] - 13s 580ms/step - loss: 0.9505 - accuracy: 0.6818 - val_loss: 2.2885 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.33333\n",
      "Epoch 11/100\n",
      "22/22 [==============================] - 13s 578ms/step - loss: 1.0980 - accuracy: 0.5909 - val_loss: 2.2699 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.33333\n",
      "Epoch 12/100\n",
      "22/22 [==============================] - 13s 579ms/step - loss: 1.3309 - accuracy: 0.5000 - val_loss: 3.0161 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.33333\n",
      "Epoch 13/100\n",
      "22/22 [==============================] - 13s 579ms/step - loss: 1.4786 - accuracy: 0.5909 - val_loss: 2.4090 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.33333\n",
      "Epoch 14/100\n",
      "22/22 [==============================] - 13s 575ms/step - loss: 0.9012 - accuracy: 0.6818 - val_loss: 2.0481 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.33333\n",
      "Epoch 15/100\n",
      "22/22 [==============================] - 13s 587ms/step - loss: 0.7417 - accuracy: 0.6818 - val_loss: 2.7037 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00015: val_accuracy improved from 0.33333 to 0.66667, saving model to intent_model.h5\n",
      "Epoch 16/100\n",
      "22/22 [==============================] - 13s 582ms/step - loss: 0.5659 - accuracy: 0.7273 - val_loss: 0.9129 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.66667\n",
      "Epoch 17/100\n",
      "22/22 [==============================] - 13s 584ms/step - loss: 1.0435 - accuracy: 0.6818 - val_loss: 3.8151 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.66667\n",
      "Epoch 18/100\n",
      "22/22 [==============================] - 13s 576ms/step - loss: 0.5147 - accuracy: 0.7273 - val_loss: 2.1818 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.66667\n",
      "Epoch 19/100\n",
      "22/22 [==============================] - 13s 577ms/step - loss: 1.9949 - accuracy: 0.6818 - val_loss: 0.2072 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00019: val_accuracy improved from 0.66667 to 1.00000, saving model to intent_model.h5\n",
      "Epoch 20/100\n",
      "22/22 [==============================] - 13s 575ms/step - loss: 1.0967 - accuracy: 0.6364 - val_loss: 1.8971 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 1.00000\n",
      "Epoch 21/100\n",
      "22/22 [==============================] - 13s 579ms/step - loss: 0.4734 - accuracy: 0.7273 - val_loss: 2.9819 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 1.00000\n",
      "Epoch 22/100\n",
      "22/22 [==============================] - 13s 574ms/step - loss: 0.3003 - accuracy: 0.8182 - val_loss: 3.1919 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 1.00000\n",
      "Epoch 23/100\n",
      "22/22 [==============================] - 13s 574ms/step - loss: 0.3759 - accuracy: 0.7727 - val_loss: 2.5244 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 1.00000\n",
      "Epoch 24/100\n",
      "22/22 [==============================] - 13s 597ms/step - loss: 0.2137 - accuracy: 0.8636 - val_loss: 2.8342 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 1.00000\n",
      "Epoch 25/100\n",
      "22/22 [==============================] - 13s 575ms/step - loss: 0.3364 - accuracy: 0.8636 - val_loss: 2.1314 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 1.00000\n",
      "Epoch 26/100\n",
      "22/22 [==============================] - 13s 576ms/step - loss: 0.2667 - accuracy: 0.8182 - val_loss: 1.4753 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 1.00000\n",
      "Epoch 27/100\n",
      "22/22 [==============================] - 13s 573ms/step - loss: 0.2315 - accuracy: 0.8182 - val_loss: 2.8018 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 1.00000\n",
      "Epoch 28/100\n",
      "22/22 [==============================] - 13s 571ms/step - loss: 0.2461 - accuracy: 0.8636 - val_loss: 4.2198 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 1.00000\n",
      "Epoch 29/100\n",
      "22/22 [==============================] - 13s 572ms/step - loss: 0.2888 - accuracy: 0.8182 - val_loss: 4.3696 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 1.00000\n",
      "Epoch 30/100\n",
      "22/22 [==============================] - 13s 571ms/step - loss: 0.2421 - accuracy: 0.8182 - val_loss: 4.2324 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 1.00000\n",
      "Epoch 31/100\n",
      "22/22 [==============================] - 13s 581ms/step - loss: 0.2072 - accuracy: 0.8636 - val_loss: 4.3002 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 1.00000\n",
      "Epoch 32/100\n",
      "22/22 [==============================] - 13s 571ms/step - loss: 0.2066 - accuracy: 0.8636 - val_loss: 4.3799 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 1.00000\n",
      "Epoch 33/100\n",
      "22/22 [==============================] - 13s 571ms/step - loss: 0.2032 - accuracy: 0.8636 - val_loss: 4.6110 - val_accuracy: 0.3333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00033: val_accuracy did not improve from 1.00000\n",
      "Epoch 34/100\n",
      "22/22 [==============================] - 13s 571ms/step - loss: 0.2183 - accuracy: 0.8182 - val_loss: 4.6073 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 1.00000\n",
      "Epoch 35/100\n",
      "22/22 [==============================] - 13s 574ms/step - loss: 0.2520 - accuracy: 0.8182 - val_loss: 4.4655 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 1.00000\n",
      "Epoch 36/100\n",
      "22/22 [==============================] - 13s 571ms/step - loss: 0.2245 - accuracy: 0.8182 - val_loss: 4.6107 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 1.00000\n",
      "Epoch 37/100\n",
      "22/22 [==============================] - 13s 572ms/step - loss: 0.2422 - accuracy: 0.8636 - val_loss: 4.1971 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 1.00000\n",
      "Epoch 38/100\n",
      "22/22 [==============================] - 13s 573ms/step - loss: 0.2010 - accuracy: 0.8636 - val_loss: 3.6902 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 1.00000\n",
      "Epoch 39/100\n",
      "22/22 [==============================] - 13s 570ms/step - loss: 0.2619 - accuracy: 0.8636 - val_loss: 4.4192 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 1.00000\n",
      "Epoch 40/100\n",
      "22/22 [==============================] - 13s 572ms/step - loss: 0.3742 - accuracy: 0.8182 - val_loss: 3.2325 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 1.00000\n",
      "Epoch 41/100\n",
      "22/22 [==============================] - 13s 572ms/step - loss: 0.2986 - accuracy: 0.7727 - val_loss: 3.1786 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 1.00000\n",
      "Epoch 42/100\n",
      "22/22 [==============================] - 13s 571ms/step - loss: 0.2666 - accuracy: 0.7727 - val_loss: 4.2901 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 1.00000\n",
      "Epoch 43/100\n",
      "22/22 [==============================] - 13s 576ms/step - loss: 0.2196 - accuracy: 0.8636 - val_loss: 4.1951 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 1.00000\n",
      "Epoch 44/100\n",
      "22/22 [==============================] - 13s 571ms/step - loss: 0.2521 - accuracy: 0.7727 - val_loss: 3.7950 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 1.00000\n",
      "Epoch 45/100\n",
      "22/22 [==============================] - 13s 573ms/step - loss: 0.2414 - accuracy: 0.7727 - val_loss: 3.5684 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 1.00000\n",
      "Epoch 46/100\n",
      "22/22 [==============================] - 13s 572ms/step - loss: 0.2929 - accuracy: 0.8636 - val_loss: 2.4074 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 1.00000\n",
      "Epoch 47/100\n",
      "22/22 [==============================] - 13s 572ms/step - loss: 0.2491 - accuracy: 0.8182 - val_loss: 2.7030 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 1.00000\n",
      "Epoch 48/100\n",
      "22/22 [==============================] - 13s 577ms/step - loss: 0.2142 - accuracy: 0.8636 - val_loss: 2.9955 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 1.00000\n",
      "Epoch 49/100\n",
      "22/22 [==============================] - 13s 570ms/step - loss: 0.2283 - accuracy: 0.8182 - val_loss: 3.0238 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 1.00000\n",
      "Epoch 50/100\n",
      "22/22 [==============================] - 13s 572ms/step - loss: 0.1985 - accuracy: 0.8636 - val_loss: 3.5296 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 1.00000\n",
      "Epoch 51/100\n",
      "22/22 [==============================] - 13s 571ms/step - loss: 0.2340 - accuracy: 0.8636 - val_loss: 3.7450 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00051: val_accuracy did not improve from 1.00000\n",
      "Epoch 52/100\n",
      "22/22 [==============================] - 13s 573ms/step - loss: 0.2488 - accuracy: 0.7273 - val_loss: 2.7573 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 1.00000\n",
      "Epoch 53/100\n",
      "22/22 [==============================] - 13s 570ms/step - loss: 0.2092 - accuracy: 0.8182 - val_loss: 2.0931 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 1.00000\n",
      "Epoch 54/100\n",
      "22/22 [==============================] - 13s 572ms/step - loss: 0.2253 - accuracy: 0.8182 - val_loss: 2.6851 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 1.00000\n",
      "Epoch 55/100\n",
      "22/22 [==============================] - 13s 575ms/step - loss: 0.2026 - accuracy: 0.8636 - val_loss: 3.0855 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 1.00000\n",
      "Epoch 56/100\n",
      "22/22 [==============================] - 13s 573ms/step - loss: 0.2115 - accuracy: 0.8182 - val_loss: 3.4790 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 1.00000\n",
      "Epoch 57/100\n",
      "22/22 [==============================] - 13s 574ms/step - loss: 0.2108 - accuracy: 0.8182 - val_loss: 3.1922 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 1.00000\n",
      "Epoch 58/100\n",
      "22/22 [==============================] - 13s 572ms/step - loss: 0.2598 - accuracy: 0.7727 - val_loss: 3.1773 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 1.00000\n",
      "Epoch 59/100\n",
      "22/22 [==============================] - 13s 576ms/step - loss: 0.2004 - accuracy: 0.8636 - val_loss: 3.2721 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00059: val_accuracy did not improve from 1.00000\n",
      "Epoch 60/100\n",
      "22/22 [==============================] - 13s 571ms/step - loss: 0.2035 - accuracy: 0.8182 - val_loss: 3.5047 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 1.00000\n",
      "Epoch 61/100\n",
      "22/22 [==============================] - 13s 570ms/step - loss: 0.2143 - accuracy: 0.8182 - val_loss: 3.5866 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00061: val_accuracy did not improve from 1.00000\n",
      "Epoch 62/100\n",
      "22/22 [==============================] - 13s 574ms/step - loss: 0.1970 - accuracy: 0.8636 - val_loss: 3.9084 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00062: val_accuracy did not improve from 1.00000\n",
      "Epoch 63/100\n",
      "22/22 [==============================] - 13s 571ms/step - loss: 0.2512 - accuracy: 0.8636 - val_loss: 4.0815 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00063: val_accuracy did not improve from 1.00000\n",
      "Epoch 64/100\n",
      "22/22 [==============================] - 13s 580ms/step - loss: 0.1952 - accuracy: 0.9091 - val_loss: 3.6740 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00064: val_accuracy did not improve from 1.00000\n",
      "Epoch 65/100\n",
      "22/22 [==============================] - 13s 571ms/step - loss: 0.2167 - accuracy: 0.8182 - val_loss: 3.5946 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00065: val_accuracy did not improve from 1.00000\n",
      "Epoch 66/100\n",
      "22/22 [==============================] - 13s 581ms/step - loss: 0.2220 - accuracy: 0.8182 - val_loss: 3.6457 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00066: val_accuracy did not improve from 1.00000\n",
      "Epoch 67/100\n",
      "22/22 [==============================] - 13s 572ms/step - loss: 0.2018 - accuracy: 0.8182 - val_loss: 3.8046 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00067: val_accuracy did not improve from 1.00000\n",
      "Epoch 68/100\n",
      "22/22 [==============================] - 13s 572ms/step - loss: 0.2037 - accuracy: 0.8636 - val_loss: 3.9850 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00068: val_accuracy did not improve from 1.00000\n",
      "Epoch 69/100\n",
      "22/22 [==============================] - 13s 574ms/step - loss: 0.2048 - accuracy: 0.8182 - val_loss: 3.8036 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00069: val_accuracy did not improve from 1.00000\n",
      "Epoch 70/100\n",
      "22/22 [==============================] - 13s 572ms/step - loss: 0.1996 - accuracy: 0.8636 - val_loss: 4.0113 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00070: val_accuracy did not improve from 1.00000\n",
      "Epoch 71/100\n",
      "22/22 [==============================] - 13s 573ms/step - loss: 0.2309 - accuracy: 0.8182 - val_loss: 4.1317 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00071: val_accuracy did not improve from 1.00000\n",
      "Epoch 72/100\n",
      "22/22 [==============================] - 13s 570ms/step - loss: 0.1995 - accuracy: 0.8182 - val_loss: 4.0094 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00072: val_accuracy did not improve from 1.00000\n",
      "Epoch 73/100\n",
      "22/22 [==============================] - 13s 571ms/step - loss: 0.2042 - accuracy: 0.8636 - val_loss: 3.9883 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00073: val_accuracy did not improve from 1.00000\n",
      "Epoch 74/100\n",
      "22/22 [==============================] - 13s 572ms/step - loss: 0.2063 - accuracy: 0.8636 - val_loss: 4.0889 - val_accuracy: 0.3333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00074: val_accuracy did not improve from 1.00000\n",
      "Epoch 75/100\n",
      "22/22 [==============================] - 13s 570ms/step - loss: 0.2173 - accuracy: 0.8182 - val_loss: 4.2453 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00075: val_accuracy did not improve from 1.00000\n",
      "Epoch 76/100\n",
      "22/22 [==============================] - 13s 574ms/step - loss: 0.2037 - accuracy: 0.7727 - val_loss: 4.0533 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00076: val_accuracy did not improve from 1.00000\n",
      "Epoch 77/100\n",
      "22/22 [==============================] - 13s 571ms/step - loss: 0.1974 - accuracy: 0.7727 - val_loss: 4.1663 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00077: val_accuracy did not improve from 1.00000\n",
      "Epoch 78/100\n",
      "22/22 [==============================] - 13s 572ms/step - loss: 0.2119 - accuracy: 0.7727 - val_loss: 4.2704 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00078: val_accuracy did not improve from 1.00000\n",
      "Epoch 79/100\n",
      "22/22 [==============================] - 13s 571ms/step - loss: 0.2061 - accuracy: 0.8182 - val_loss: 4.4605 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00079: val_accuracy did not improve from 1.00000\n",
      "Epoch 80/100\n",
      "22/22 [==============================] - 13s 572ms/step - loss: 0.1966 - accuracy: 0.8182 - val_loss: 4.1353 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00080: val_accuracy did not improve from 1.00000\n",
      "Epoch 81/100\n",
      "22/22 [==============================] - 13s 582ms/step - loss: 0.2039 - accuracy: 0.8182 - val_loss: 4.2307 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00081: val_accuracy did not improve from 1.00000\n",
      "Epoch 82/100\n",
      "22/22 [==============================] - 13s 572ms/step - loss: 0.1975 - accuracy: 0.8182 - val_loss: 3.7663 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00082: val_accuracy did not improve from 1.00000\n",
      "Epoch 83/100\n",
      "22/22 [==============================] - 13s 575ms/step - loss: 0.2080 - accuracy: 0.8636 - val_loss: 3.7385 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00083: val_accuracy did not improve from 1.00000\n",
      "Epoch 84/100\n",
      "22/22 [==============================] - 13s 571ms/step - loss: 0.1991 - accuracy: 0.8636 - val_loss: 3.7794 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00084: val_accuracy did not improve from 1.00000\n",
      "Epoch 85/100\n",
      "22/22 [==============================] - 13s 575ms/step - loss: 0.1951 - accuracy: 0.8182 - val_loss: 3.9986 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00085: val_accuracy did not improve from 1.00000\n",
      "Epoch 86/100\n",
      "22/22 [==============================] - 13s 573ms/step - loss: 0.2100 - accuracy: 0.8636 - val_loss: 3.9687 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00086: val_accuracy did not improve from 1.00000\n",
      "Epoch 87/100\n",
      "22/22 [==============================] - 13s 570ms/step - loss: 0.2405 - accuracy: 0.8182 - val_loss: 3.0560 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00087: val_accuracy did not improve from 1.00000\n",
      "Epoch 88/100\n",
      "22/22 [==============================] - 13s 572ms/step - loss: 0.2067 - accuracy: 0.8636 - val_loss: 3.3533 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00088: val_accuracy did not improve from 1.00000\n",
      "Epoch 89/100\n",
      "22/22 [==============================] - 13s 573ms/step - loss: 0.2258 - accuracy: 0.8182 - val_loss: 3.5442 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00089: val_accuracy did not improve from 1.00000\n",
      "Epoch 90/100\n",
      "22/22 [==============================] - 15s 690ms/step - loss: 0.2075 - accuracy: 0.8636 - val_loss: 3.8758 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00090: val_accuracy did not improve from 1.00000\n",
      "Epoch 91/100\n",
      "22/22 [==============================] - 17s 782ms/step - loss: 0.2059 - accuracy: 0.8182 - val_loss: 2.6435 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00091: val_accuracy did not improve from 1.00000\n",
      "Epoch 92/100\n",
      "22/22 [==============================] - 14s 615ms/step - loss: 0.2194 - accuracy: 0.8636 - val_loss: 2.6131 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00092: val_accuracy did not improve from 1.00000\n",
      "Epoch 93/100\n",
      "22/22 [==============================] - 15s 675ms/step - loss: 0.2003 - accuracy: 0.8182 - val_loss: 3.2159 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00093: val_accuracy did not improve from 1.00000\n",
      "Epoch 94/100\n",
      "22/22 [==============================] - 13s 603ms/step - loss: 0.2158 - accuracy: 0.8636 - val_loss: 3.4796 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00094: val_accuracy did not improve from 1.00000\n",
      "Epoch 95/100\n",
      "22/22 [==============================] - 13s 601ms/step - loss: 0.2132 - accuracy: 0.8182 - val_loss: 2.6827 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00095: val_accuracy did not improve from 1.00000\n",
      "Epoch 96/100\n",
      "22/22 [==============================] - 13s 609ms/step - loss: 0.1986 - accuracy: 0.8636 - val_loss: 2.8629 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00096: val_accuracy did not improve from 1.00000\n",
      "Epoch 97/100\n",
      "22/22 [==============================] - 13s 608ms/step - loss: 0.2007 - accuracy: 0.8636 - val_loss: 2.9658 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00097: val_accuracy did not improve from 1.00000\n",
      "Epoch 98/100\n",
      "22/22 [==============================] - 13s 592ms/step - loss: 0.2194 - accuracy: 0.7727 - val_loss: 3.3251 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00098: val_accuracy did not improve from 1.00000\n",
      "Epoch 99/100\n",
      "22/22 [==============================] - 13s 608ms/step - loss: 0.2058 - accuracy: 0.8182 - val_loss: 3.1591 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00099: val_accuracy did not improve from 1.00000\n",
      "Epoch 100/100\n",
      "22/22 [==============================] - 13s 606ms/step - loss: 0.2002 - accuracy: 0.8182 - val_loss: 3.3149 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00100: val_accuracy did not improve from 1.00000\n",
      "3/3 [==============================] - 2s 547ms/step\n",
      "\n",
      "\n",
      "-------------------   Model Evaluation . ----------------------\n",
      "\n",
      " validation accuracy =  1.0  , validation loss =  0.20717214047908783 \n",
      "\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "hist, m = train_intent_classifier_model(\"multihead model\", weights_file_name, epoch_number1, vocab_size, max_length, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qSTEzrlzcuya"
   },
   "outputs": [],
   "source": [
    "\n",
    "def predictions(text, m, classes):\n",
    "  clean = re.sub(r'[^ a-z A-Z 0-9]', \" \", text)\n",
    "  test_word = word_tokenize(clean)\n",
    "  test_word = [w.lower() for w in test_word]\n",
    "  test_ls = word_tokenizer.texts_to_sequences(test_word)\n",
    "  print(test_word)\n",
    "  #Check for unknown words\n",
    "  if [] in test_ls:\n",
    "    test_ls = list(filter(None, test_ls))\n",
    "    \n",
    "  test_ls = np.array(test_ls).reshape(1, len(test_ls))\n",
    " \n",
    "  x = padding_doc(test_ls, max_length)\n",
    "  \n",
    "  pred = m.predict_proba(x)\n",
    "  \n",
    "  \n",
    "  predictions = pred[0]\n",
    " \n",
    "  classes = np.array(classes)\n",
    "  ids = np.argsort(-predictions)\n",
    "  print(ids)\n",
    "  classes = classes[ids]\n",
    "  predictions = -np.sort(-predictions)\n",
    "  clear_output()\n",
    "  for i in range(pred.shape[1]):\n",
    "    print(\"%s has confidence = %s\" % (classes[i], (predictions[i])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "23VpGuihMdEU",
    "outputId": "cd36c932-0fb0-4166-92ae-546a7676e645"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_test3 has confidence = 0.99495924\n",
      "_test1 has confidence = 0.0039426223\n",
      "_test4 has confidence = 0.0010607963\n",
      "_test2 has confidence = 3.0151312e-05\n",
      "_test5 has confidence = 6.629872e-06\n",
      "_test9 has confidence = 5.075522e-07\n",
      "_test7 has confidence = 7.535429e-08\n",
      "_test6 has confidence = 1.94949e-10\n"
     ]
    }
   ],
   "source": [
    "m = create_model(vocab_size, max_length, y_train)\n",
    "m.load_weights(weights_file_name)\n",
    "text = \"text's intent prediction\"\n",
    "predictions(text, m, unique_intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "3/3 [==============================] - 1s 473ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.20717214047908783, 1.0]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "m.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Intent_classification_final.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
